{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOR_MATCHING_PATH = 'DatasetForMatching'\n",
    "IMAGE_FOR_PREDICTION_PATH = 'DatasetForPredict/10000forPredict.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_match_percentage(img1, img2, feature_detector):\n",
    "    \"\"\"Compute the match percentage between two images using keypoints and RANSAC.\"\"\"\n",
    "    \n",
    "    # Detect keypoints and compute descriptors for both images\n",
    "    keypoints1, descriptors1 = feature_detector.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = feature_detector.detectAndCompute(img2, None)\n",
    "\n",
    "    if isinstance(feature_detector, cv2.ORB):  # ORB uses binary descriptors (Hamming distance)\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(descriptors1, descriptors2)\n",
    "    else:\n",
    "        # For SIFT or other detectors, use FLANN (descriptors are float32)\n",
    "        index_params = dict(algorithm=1, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "    # Lowe's ratio test (only for FLANN-based matching)\n",
    "    if not isinstance(feature_detector, cv2.ORB):\n",
    "        good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "    else:\n",
    "        good_matches = matches  # No need for ratio test in BFMatcher with crossCheck=True\n",
    "\n",
    "    # Estimate homography using RANSAC if there are enough good matches\n",
    "    valid_matches = 0\n",
    "    if len(good_matches) >= 4:\n",
    "        src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        _, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if mask is not None:\n",
    "            valid_matches = sum(mask.ravel())\n",
    "\n",
    "    total_keypoints = len(keypoints1)\n",
    "    match_percentage = (valid_matches / total_keypoints * 100) if total_keypoints > 0 else 0\n",
    "    return len(keypoints1), valid_matches, match_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('fcnn_model.h5')\n",
    "# model = joblib.load('random_forest_model.pkl')\n",
    "loaded_encoder = joblib.load('label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menggabungkan fitur dan menggunakan model untuk prediksi\n",
    "def generate_prediction(matching_images_path):\n",
    "    matching_images = {}\n",
    "    \n",
    "    # Memuat gambar untuk pencocokan\n",
    "    for f in os.listdir(IMAGE_FOR_MATCHING_PATH):\n",
    "        if f.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(matching_images_path, f)\n",
    "            preprocessed_image = preprocess_image(img_path)\n",
    "            matching_images[os.path.basename(f)] = preprocessed_image\n",
    "\n",
    "    # Proses gambar dalam dataset\n",
    "\n",
    "    img_path = IMAGE_FOR_PREDICTION_PATH\n",
    "    img_preprocessed = preprocess_image(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Mengumpulkan metadata gambar\n",
    "    img_name = os.path.basename(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    total_pixels = height * width\n",
    "    avg_color_R, avg_color_G, avg_color_B = img.mean(axis=(0, 1)).tolist()\n",
    "\n",
    "    # Hasil pencocokan SIFT\n",
    "    sift_match_results = {}\n",
    "    for match_name, match_img in matching_images.items():\n",
    "        _, _, match_percentage = compute_match_percentage(img_preprocessed, match_img, feature_detector=cv2.SIFT_create())\n",
    "        sift_match_results[f\"sift_match_percentage_with_{match_name}\"] = match_percentage\n",
    "\n",
    "    # Hasil pencocokan ORB\n",
    "    orb_match_results = {}\n",
    "    for match_name, match_img in matching_images.items():\n",
    "        _, _, match_percentage = compute_match_percentage(img_preprocessed, match_img, feature_detector=cv2.ORB_create())\n",
    "        orb_match_results[f\"orb_match_percentage_with_{match_name}\"] = match_percentage\n",
    "\n",
    "    # Menggabungkan semua fitur\n",
    "    features = {\n",
    "        \"total_pixels\": total_pixels,\n",
    "        \"avg_color_R\": avg_color_R,\n",
    "        \"avg_color_G\": avg_color_G,\n",
    "        \"avg_color_B\": avg_color_B,\n",
    "        **sift_match_results,\n",
    "        **orb_match_results\n",
    "    }\n",
    "\n",
    "    # Memasukkan fitur ke dalam model dan mendapatkan prediksi\n",
    "    input_features = np.array(list(features.values()), dtype=np.float32).reshape(1, -1)\n",
    "    prediction = model.predict(input_features)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    print(predicted_index)\n",
    "    print(f\"{img_name}: {loaded_encoder.inverse_transform([predicted_index])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction for DatasetForPredict...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "6\n",
      "10000forPredict.jpg: [100000]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating prediction for DatasetForPredict...\")\n",
    "generate_prediction(IMAGE_FOR_MATCHING_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
